{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import librairies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20110, 4)\n",
      "(46924, 4)\n",
      "(67034, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_final = pd.read_csv('/workspace/data/final_diag50.csv')\n",
    "#batching data into samples\n",
    "df_final1 = df_final.sample(frac=0.3)\n",
    "df_final2 = df_final.sample(frac=0.7)\n",
    "df_final3 = df_final.sample(frac=1)\n",
    "#checking size\n",
    "print(df_final1.shape)\n",
    "print(df_final2.shape)\n",
    "print(df_final3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 30% of the dataset\n",
    "data1 = df_final1.copy()\n",
    "# Load 70% of the dataset\n",
    "data2 = df_final2.copy()\n",
    "# Load full dataset\n",
    "data3 = df_final3.copy()\n",
    "#train/test split was 80/20%\n",
    "# Split the dataset into training and testing sets\n",
    "train_df1, test_df1 = train_test_split(data1, test_size=0.2, random_state=42)\n",
    "train_df2, test_df2 = train_test_split(data2, test_size=0.2, random_state=42)\n",
    "train_df3, test_df3 = train_test_split(data3, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the ClinicalBERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "train_encodings1 = tokenize_texts(train_df1['clean_text'].tolist())\n",
    "test_encodings1 = tokenize_texts(test_df1['clean_text'].tolist())\n",
    "train_encodings2 = tokenize_texts(train_df2['clean_text'].tolist())\n",
    "test_encodings2 = tokenize_texts(test_df2['clean_text'].tolist())\n",
    "train_encodings3 = tokenize_texts(train_df3['clean_text'].tolist())\n",
    "test_encodings3 = tokenize_texts(test_df3['clean_text'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on first sample=30% dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TextDataset(train_encodings1, train_df1['labels'].tolist())\n",
    "test_dataset = TextDataset(test_encodings1, test_df1['labels'].tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setting models definition\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        return self.fc(output[:, -1, :])\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        return self.fc(output[:, -1, :])\n",
    "\n",
    "class BERTForClassification(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        return self.fc(outputs['pooler_output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Time: 48.929654121398926 seconds, Testing Time: 7.71307635307312 seconds\n",
      "Epoch: 1, Training Time: 48.75060176849365 seconds, Testing Time: 7.745631456375122 seconds\n",
      "Epoch: 2, Training Time: 48.88056039810181 seconds, Testing Time: 7.739435911178589 seconds\n",
      "Epoch: 3, Training Time: 48.550743103027344 seconds, Testing Time: 7.671591520309448 seconds\n",
      "Epoch: 4, Training Time: 48.7392373085022 seconds, Testing Time: 7.772187232971191 seconds\n",
      "Epoch: 5, Training Time: 48.98300004005432 seconds, Testing Time: 7.7655980587005615 seconds\n",
      "Epoch: 6, Training Time: 48.68782591819763 seconds, Testing Time: 7.665295124053955 seconds\n",
      "Epoch: 7, Training Time: 48.916528940200806 seconds, Testing Time: 7.774595022201538 seconds\n",
      "Early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      0.04      0.05        51\n",
      "           1       0.02      0.02      0.02        56\n",
      "           2       0.02      0.04      0.03        68\n",
      "           3       0.00      0.00      0.00        48\n",
      "           4       0.04      0.05      0.05        78\n",
      "           5       0.01      0.06      0.02        54\n",
      "           6       0.03      0.05      0.04        21\n",
      "           7       0.15      0.03      0.05       562\n",
      "           8       0.01      0.02      0.02        47\n",
      "           9       0.02      0.07      0.03        42\n",
      "          10       0.04      0.12      0.05        43\n",
      "          11       0.06      0.03      0.04        67\n",
      "          12       0.02      0.05      0.03        38\n",
      "          13       0.02      0.06      0.03        67\n",
      "          14       0.01      0.01      0.01        68\n",
      "          15       0.06      0.03      0.04       202\n",
      "          16       0.11      0.08      0.09       256\n",
      "          17       0.06      0.03      0.04        71\n",
      "          18       0.00      0.00      0.00        49\n",
      "          19       0.03      0.07      0.04        95\n",
      "          20       0.00      0.00      0.00        50\n",
      "          21       0.02      0.02      0.02        56\n",
      "          22       0.06      0.03      0.04        73\n",
      "          23       0.00      0.00      0.00        59\n",
      "          24       0.01      0.03      0.01        35\n",
      "          25       0.00      0.00      0.00        91\n",
      "          26       0.05      0.03      0.04       141\n",
      "          27       0.00      0.00      0.00        43\n",
      "          28       0.00      0.00      0.00        28\n",
      "          29       0.00      0.00      0.00        18\n",
      "          30       0.03      0.02      0.02       165\n",
      "          31       0.00      0.00      0.00        73\n",
      "          32       0.05      0.03      0.04       136\n",
      "          33       0.07      0.06      0.06       310\n",
      "          34       0.00      0.00      0.00        91\n",
      "          35       0.00      0.00      0.00        19\n",
      "          36       0.07      0.05      0.06        41\n",
      "          37       0.00      0.00      0.00        29\n",
      "          38       0.03      0.03      0.03        32\n",
      "          39       0.05      0.09      0.06       122\n",
      "          40       0.00      0.00      0.00        55\n",
      "          41       0.02      0.02      0.02       105\n",
      "          42       0.00      0.00      0.00        42\n",
      "          43       0.00      0.00      0.00        62\n",
      "          44       0.03      0.04      0.03        46\n",
      "          45       0.00      0.00      0.00        45\n",
      "          46       0.00      0.00      0.00        24\n",
      "          47       0.00      0.00      0.00        15\n",
      "          48       0.00      0.00      0.00        24\n",
      "          49       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.03      4022\n",
      "   macro avg       0.03      0.03      0.02      4022\n",
      "weighted avg       0.05      0.03      0.04      4022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RNN Tuned MODEL FOR SAMPLE 1\n",
    "import torch\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "class AdjustedRNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.4):\n",
    "        super(AdjustedRNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)  # Use nn.RNN instead of nn.LSTM\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        rnn_out, _ = self.rnn(embedded)  # Use rnn_out instead of lstm_out\n",
    "        # Use the last hidden state for classification\n",
    "        output = self.fc(self.dropout(rnn_out[:, -1, :]))\n",
    "        return output\n",
    "\n",
    "# Instantiate model\n",
    "model = AdjustedRNNModel(vocab_size=tokenizer.vocab_size, embedding_dim=128, hidden_dim=256, output_dim=50, dropout=0.4).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.00002)\n",
    "\n",
    "\n",
    "# Assuming `train_labels` is a list containing all the labels in your training dataset\n",
    "train_labels = [label for batch in train_loader for label in batch['labels'].tolist()]\n",
    "\n",
    "# 1. Compute class distribution\n",
    "class_counts = Counter(train_labels)\n",
    "\n",
    "# 2. Calculate the weights\n",
    "max_count = max(class_counts.values())\n",
    "class_weights = {class_id: max_count / count for class_id, count in class_counts.items()}\n",
    "weights = [class_weights[class_id] for class_id in sorted(class_weights.keys())]\n",
    "\n",
    "weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# 3. Use the weights in the loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()  # use this if data is not imbalanced\n",
    "\n",
    "# Add Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "# Train the model\n",
    "best_f1 = 0.0  # for early stopping based on F1 score\n",
    "\n",
    "for epoch in range(10):\n",
    "    # Measure the time at the start of the epoch\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    training_start_time = time.time()  # Start measuring time\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    training_end_time = time.time()  # End measuring time\n",
    "    training_time = training_end_time - training_start_time  # Calculate elapsed time for training\n",
    "\n",
    "    model.eval()\n",
    "    testing_start_time = time.time()  # Start measuring time\n",
    "    predictions, true_labels = [], []\n",
    "    val_loss = 0  # to compute average validation loss for scheduler\n",
    "\n",
    "    for batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            predictions.extend(torch.argmax(outputs, dim=1).cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    testing_end_time = time.time()  # End measuring time\n",
    "    testing_time = testing_end_time - testing_start_time  # Calculate elapsed time for testing\n",
    "\n",
    "    val_f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    scheduler.step(val_loss / len(test_loader))  # scheduler step based on avg val loss\n",
    "    report1 = classification_report(true_labels, predictions)\n",
    "    print(f\"Epoch: {epoch}, Training Time: {training_time} seconds, Testing Time: {testing_time} seconds\")\n",
    "    # Implementing early stopping based on F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        #torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= 3:\n",
    "            print(\"Early stopping\")\n",
    "            print(report1)\n",
    "            break\n",
    "\n",
    "\n",
    "else:  # This block will be executed if the for loop completes normally, i.e., if early stopping does not occur.\n",
    "    print(\"Training completed.\")\n",
    "    print(report1)  # Print the classification report after the last epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Time: 120.55844593048096 seconds, Testing Time: 16.22673225402832 seconds\n",
      "Epoch: 1, Training Time: 120.63873291015625 seconds, Testing Time: 16.181991815567017 seconds\n",
      "Epoch: 2, Training Time: 125.98930525779724 seconds, Testing Time: 17.474894285202026 seconds\n",
      "Epoch: 3, Training Time: 122.63699316978455 seconds, Testing Time: 16.10278820991516 seconds\n",
      "Epoch: 4, Training Time: 120.7151620388031 seconds, Testing Time: 16.190575122833252 seconds\n",
      "Epoch: 5, Training Time: 120.8771026134491 seconds, Testing Time: 16.11627984046936 seconds\n",
      "Epoch: 6, Training Time: 121.14670324325562 seconds, Testing Time: 16.228591918945312 seconds\n",
      "Epoch: 7, Training Time: 120.92098712921143 seconds, Testing Time: 16.175971031188965 seconds\n",
      "Epoch: 8, Training Time: 121.5137414932251 seconds, Testing Time: 16.196144580841064 seconds\n",
      "Epoch: 9, Training Time: 121.9709324836731 seconds, Testing Time: 16.269481658935547 seconds\n",
      "Training completed.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.08      0.08        51\n",
      "           1       0.08      0.14      0.10        56\n",
      "           2       0.12      0.21      0.16        68\n",
      "           3       0.15      0.21      0.18        48\n",
      "           4       0.11      0.14      0.12        78\n",
      "           5       0.08      0.17      0.11        54\n",
      "           6       0.06      0.24      0.10        21\n",
      "           7       0.31      0.05      0.08       562\n",
      "           8       0.19      0.28      0.23        47\n",
      "           9       0.20      0.17      0.18        42\n",
      "          10       0.19      0.26      0.22        43\n",
      "          11       0.21      0.25      0.23        67\n",
      "          12       0.15      0.18      0.16        38\n",
      "          13       0.14      0.15      0.14        67\n",
      "          14       0.18      0.22      0.20        68\n",
      "          15       0.24      0.14      0.18       202\n",
      "          16       0.45      0.32      0.37       256\n",
      "          17       0.13      0.21      0.16        71\n",
      "          18       0.14      0.16      0.15        49\n",
      "          19       0.21      0.28      0.24        95\n",
      "          20       0.27      0.36      0.31        50\n",
      "          21       0.11      0.14      0.12        56\n",
      "          22       0.11      0.19      0.14        73\n",
      "          23       0.12      0.17      0.14        59\n",
      "          24       0.06      0.06      0.06        35\n",
      "          25       0.27      0.40      0.32        91\n",
      "          26       0.29      0.29      0.29       141\n",
      "          27       0.31      0.30      0.31        43\n",
      "          28       0.04      0.04      0.04        28\n",
      "          29       0.09      0.28      0.13        18\n",
      "          30       0.15      0.11      0.12       165\n",
      "          31       0.10      0.12      0.11        73\n",
      "          32       0.11      0.10      0.10       136\n",
      "          33       0.20      0.07      0.10       310\n",
      "          34       0.08      0.16      0.10        91\n",
      "          35       0.02      0.05      0.03        19\n",
      "          36       0.21      0.20      0.20        41\n",
      "          37       0.12      0.17      0.14        29\n",
      "          38       0.09      0.25      0.13        32\n",
      "          39       0.19      0.17      0.18       122\n",
      "          40       0.10      0.13      0.11        55\n",
      "          41       0.11      0.11      0.11       105\n",
      "          42       0.13      0.14      0.14        42\n",
      "          43       0.15      0.16      0.16        62\n",
      "          44       0.08      0.13      0.10        46\n",
      "          45       0.14      0.16      0.15        45\n",
      "          46       0.09      0.21      0.13        24\n",
      "          47       0.08      0.13      0.10        15\n",
      "          48       0.32      0.25      0.28        24\n",
      "          49       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.16      4022\n",
      "   macro avg       0.15      0.18      0.16      4022\n",
      "weighted avg       0.20      0.16      0.16      4022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##LSTM tined MODEL FOR SAMPLE 1\n",
    "\n",
    "import warnings\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class AdjustedLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.2):\n",
    "        super(AdjustedLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # Use the last hidden state for classification\n",
    "        output = self.fc(self.dropout(lstm_out[:, -1, :]))\n",
    "        return output\n",
    "\n",
    "# Instantiate model\n",
    "model = AdjustedLSTMModel(vocab_size=tokenizer.vocab_size, embedding_dim=128, hidden_dim=256, output_dim=50, dropout=0.2).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(),lr=0.001)\n",
    "\n",
    "# If your dataset is imbalanced, compute class weights\n",
    "# weights = # Compute based on class distribution\n",
    "# criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "\n",
    "# Assuming `train_labels` is a list containing all the labels in your training dataset\n",
    "train_labels = [label for batch in train_loader for label in batch['labels'].tolist()]\n",
    "\n",
    "# 1. Compute class distribution\n",
    "class_counts = Counter(train_labels)\n",
    "\n",
    "# 2. Calculate the weights\n",
    "max_count = max(class_counts.values())\n",
    "class_weights = {class_id: max_count / count for class_id, count in class_counts.items()}\n",
    "weights = [class_weights[class_id] for class_id in sorted(class_weights.keys())]\n",
    "\n",
    "weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# 3. Use the weights in the loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()  # use this if data is not imbalanced\n",
    "\n",
    "# Add Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "# Train the model\n",
    "best_f1 = 0.0  # for early stopping based on F1 score\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    training_start_time = time.time()  # Start measuring time\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    training_end_time = time.time()  # End measuring time\n",
    "    training_time = training_end_time - training_start_time  # Calculate elapsed time for training\n",
    "\n",
    "    model.eval()\n",
    "    testing_start_time = time.time()  # Start measuring time\n",
    "    predictions, true_labels = [], []\n",
    "    val_loss = 0  # to compute average validation loss for scheduler\n",
    "\n",
    "    for batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            predictions.extend(torch.argmax(outputs, dim=1).cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    testing_end_time = time.time()  # End measuring time\n",
    "    testing_time = testing_end_time - testing_start_time  # Calculate elapsed time for testing\n",
    "\n",
    "    val_f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    scheduler.step(val_loss / len(test_loader))  # scheduler step based on avg val loss\n",
    "    report2 = classification_report(true_labels, predictions)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Training Time: {training_time} seconds, Testing Time: {testing_time} seconds\")\n",
    "    # Implementing early stopping based on F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        #torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= 3:\n",
    "            print(\"Early stopping\")\n",
    "            print(report2)\n",
    "            break\n",
    "\n",
    "else:  # This block will be executed if the for loop completes normally, i.e., if early stopping does not occur.\n",
    "    print(\"Training completed.\")\n",
    "    print(report2)  # Print the classification report after the last epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Time: 146.2873899936676 seconds, Testing Time: 16.79489278793335 seconds\n",
      "Epoch: 1, Training Time: 153.2348906993866 seconds, Testing Time: 17.7624249458313 seconds\n",
      "Epoch: 2, Training Time: 145.9025137424469 seconds, Testing Time: 16.88110375404358 seconds\n",
      "Epoch: 3, Training Time: 146.2739658355713 seconds, Testing Time: 16.773165464401245 seconds\n",
      "Epoch: 4, Training Time: 145.75145030021667 seconds, Testing Time: 16.625985860824585 seconds\n",
      "Epoch: 5, Training Time: 144.3977952003479 seconds, Testing Time: 16.76985812187195 seconds\n",
      "Epoch: 6, Training Time: 143.4182629585266 seconds, Testing Time: 16.611148595809937 seconds\n",
      "Epoch: 7, Training Time: 143.43671202659607 seconds, Testing Time: 16.652474403381348 seconds\n",
      "Epoch: 8, Training Time: 144.37139749526978 seconds, Testing Time: 16.579089164733887 seconds\n",
      "Epoch: 9, Training Time: 145.12491631507874 seconds, Testing Time: 17.73709535598755 seconds\n",
      "Training completed.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.18      0.20        51\n",
      "           1       0.33      0.25      0.28        56\n",
      "           2       0.31      0.26      0.28        68\n",
      "           3       0.38      0.35      0.37        48\n",
      "           4       0.26      0.38      0.31        78\n",
      "           5       0.21      0.26      0.23        54\n",
      "           6       0.25      0.29      0.27        21\n",
      "           7       0.40      0.26      0.31       562\n",
      "           8       0.22      0.45      0.29        47\n",
      "           9       0.75      0.64      0.69        42\n",
      "          10       0.42      0.30      0.35        43\n",
      "          11       0.63      0.51      0.56        67\n",
      "          12       0.43      0.42      0.43        38\n",
      "          13       0.29      0.27      0.28        67\n",
      "          14       0.25      0.31      0.27        68\n",
      "          15       0.31      0.28      0.29       202\n",
      "          16       0.65      0.57      0.61       256\n",
      "          17       0.37      0.44      0.40        71\n",
      "          18       0.66      0.55      0.60        49\n",
      "          19       0.51      0.74      0.60        95\n",
      "          20       0.69      0.58      0.63        50\n",
      "          21       0.28      0.30      0.29        56\n",
      "          22       0.28      0.33      0.30        73\n",
      "          23       0.20      0.25      0.23        59\n",
      "          24       0.10      0.09      0.09        35\n",
      "          25       0.76      0.68      0.72        91\n",
      "          26       0.56      0.74      0.64       141\n",
      "          27       0.64      0.37      0.47        43\n",
      "          28       0.27      0.25      0.26        28\n",
      "          29       0.21      0.22      0.22        18\n",
      "          30       0.24      0.30      0.27       165\n",
      "          31       0.30      0.19      0.24        73\n",
      "          32       0.32      0.15      0.21       136\n",
      "          33       0.32      0.42      0.36       310\n",
      "          34       0.33      0.41      0.36        91\n",
      "          35       0.44      0.21      0.29        19\n",
      "          36       0.50      0.49      0.49        41\n",
      "          37       0.38      0.41      0.39        29\n",
      "          38       0.45      0.44      0.44        32\n",
      "          39       0.48      0.65      0.55       122\n",
      "          40       0.27      0.33      0.30        55\n",
      "          41       0.23      0.25      0.24       105\n",
      "          42       0.48      0.33      0.39        42\n",
      "          43       0.49      0.50      0.50        62\n",
      "          44       0.24      0.26      0.25        46\n",
      "          45       0.46      0.40      0.43        45\n",
      "          46       0.21      0.38      0.27        24\n",
      "          47       0.40      0.67      0.50        15\n",
      "          48       0.59      0.42      0.49        24\n",
      "          49       0.50      0.22      0.31         9\n",
      "\n",
      "    accuracy                           0.38      4022\n",
      "   macro avg       0.39      0.38      0.38      4022\n",
      "weighted avg       0.40      0.38      0.38      4022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##BiLSTM tuned MODEL FOR SAMPLE 1\n",
    "import warnings\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class AdjustedBiLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.2):\n",
    "        super(AdjustedBiLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Add this line to store the hidden_dim as an instance variable\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_dim)  # Multiply by 2 because it's bidirectional\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # Use the last hidden state for classification. We concatenate the last hidden state from both directions\n",
    "        output = self.fc(self.dropout(torch.cat((lstm_out[:, -1, :self.hidden_dim], lstm_out[:, 0, self.hidden_dim:]), dim=1)))\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "model = AdjustedBiLSTMModel(vocab_size=tokenizer.vocab_size, embedding_dim=128, hidden_dim=256, output_dim=50, dropout=0.2).to(device)\n",
    "\n",
    "#model = AdjustedLSTMModel(vocab_size=tokenizer.vocab_size, embedding_dim=128, hidden_dim=256, output_dim=10, dropout=0.2).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(),lr=0.001)\n",
    "\n",
    "# If your dataset is imbalanced, compute class weights\n",
    "# weights = # Compute based on class distribution\n",
    "# criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "\n",
    "# Assuming `train_labels` is a list containing all the labels in your training dataset\n",
    "train_labels = [label for batch in train_loader for label in batch['labels'].tolist()]\n",
    "\n",
    "# 1. Compute class distribution\n",
    "class_counts = Counter(train_labels)\n",
    "\n",
    "# 2. Calculate the weights\n",
    "max_count = max(class_counts.values())\n",
    "class_weights = {class_id: max_count / count for class_id, count in class_counts.items()}\n",
    "weights = [class_weights[class_id] for class_id in sorted(class_weights.keys())]\n",
    "\n",
    "weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# 3. Use the weights in the loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()  # use this if data is not imbalanced\n",
    "\n",
    "# Add Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "# Train the model\n",
    "best_f1 = 0.0  # for early stopping based on F1 score\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    training_start_time = time.time()  # Start measuring time\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    training_end_time = time.time()  # End measuring time\n",
    "    training_time = training_end_time - training_start_time  # Calculate elapsed time for training\n",
    "\n",
    "    model.eval()\n",
    "    testing_start_time = time.time()  # Start measuring time\n",
    "    predictions, true_labels = [], []\n",
    "    val_loss = 0  # to compute average validation loss for scheduler\n",
    "\n",
    "    for batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            predictions.extend(torch.argmax(outputs, dim=1).cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    testing_end_time = time.time()  # End measuring time\n",
    "    testing_time = testing_end_time - testing_start_time  # Calculate elapsed time for testing\n",
    "\n",
    "    val_f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    scheduler.step(val_loss / len(test_loader))  # scheduler step based on avg val loss\n",
    "    report3 = classification_report(true_labels, predictions)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Training Time: {training_time} seconds, Testing Time: {testing_time} seconds\")\n",
    "    \n",
    "    # Implementing early stopping based on F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        #torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= 3:\n",
    "            print(\"Early stopping\")\n",
    "            print(report3)\n",
    "            break\n",
    "\n",
    "else:  # This block will be executed if the for loop completes normally, i.e., if early stopping does not occur.\n",
    "    print(\"Training completed.\")\n",
    "    print(report3)  # Print the classification report after the last epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Time: 2336.5735466480255 seconds, Testing Time: 221.35874676704407 seconds\n",
      "Epoch: 1, Training Time: 3212.2437999248505 seconds, Testing Time: 445.291729927063 seconds\n",
      "Epoch: 2, Training Time: 4971.916688919067 seconds, Testing Time: 445.376238822937 seconds\n",
      "Epoch: 3, Training Time: 4966.547714710236 seconds, Testing Time: 471.38681292533875 seconds\n",
      "Early stopping triggered.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.33      0.37        51\n",
      "           1       0.54      0.50      0.52        56\n",
      "           2       0.42      0.46      0.44        68\n",
      "           3       0.86      0.62      0.72        48\n",
      "           4       0.49      0.31      0.38        78\n",
      "           5       0.62      0.15      0.24        54\n",
      "           6       0.50      0.10      0.16        21\n",
      "           7       0.60      0.67      0.63       562\n",
      "           8       0.57      0.49      0.53        47\n",
      "           9       0.87      0.98      0.92        42\n",
      "          10       0.90      0.81      0.85        43\n",
      "          11       0.85      0.87      0.86        67\n",
      "          12       0.62      0.61      0.61        38\n",
      "          13       0.28      0.07      0.12        67\n",
      "          14       0.38      0.71      0.49        68\n",
      "          15       0.47      0.60      0.53       202\n",
      "          16       0.79      0.75      0.77       256\n",
      "          17       0.69      0.80      0.74        71\n",
      "          18       0.90      0.78      0.84        49\n",
      "          19       0.84      0.71      0.77        95\n",
      "          20       0.93      0.82      0.87        50\n",
      "          21       0.69      0.48      0.57        56\n",
      "          22       0.70      0.42      0.53        73\n",
      "          23       0.42      0.51      0.46        59\n",
      "          24       0.20      0.06      0.09        35\n",
      "          25       0.93      0.95      0.94        91\n",
      "          26       0.78      0.88      0.83       141\n",
      "          27       0.70      0.44      0.54        43\n",
      "          28       0.46      0.43      0.44        28\n",
      "          29       0.70      0.78      0.74        18\n",
      "          30       0.50      0.61      0.54       165\n",
      "          31       0.73      0.51      0.60        73\n",
      "          32       0.52      0.42      0.47       136\n",
      "          33       0.56      0.75      0.64       310\n",
      "          34       0.73      0.35      0.47        91\n",
      "          35       0.74      0.74      0.74        19\n",
      "          36       0.54      0.80      0.65        41\n",
      "          37       0.70      0.24      0.36        29\n",
      "          38       0.42      0.44      0.43        32\n",
      "          39       0.74      0.87      0.80       122\n",
      "          40       0.60      0.78      0.68        55\n",
      "          41       0.59      0.52      0.56       105\n",
      "          42       0.74      0.81      0.77        42\n",
      "          43       0.79      0.81      0.80        62\n",
      "          44       0.67      0.26      0.38        46\n",
      "          45       0.50      0.67      0.57        45\n",
      "          46       0.52      0.50      0.51        24\n",
      "          47       0.00      0.00      0.00        15\n",
      "          48       0.50      1.00      0.67        24\n",
      "          49       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.62      4022\n",
      "   macro avg       0.60      0.56      0.56      4022\n",
      "weighted avg       0.63      0.62      0.61      4022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##BERT MODEL FOR SAMPLE 1\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BERTForClassification(50).to(device)  # replace NUM_CLASSES with the number of unique labels in your dataset\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 3\n",
    "best_valid_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    training_start_time = time.time()  # Start measuring time\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        inputs, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "        outputs = model(inputs, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    training_end_time = time.time()  # End measuring time\n",
    "    training_time = training_end_time - training_start_time  # Calculate elapsed time for training\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    testing_start_time = time.time()  # Start measuring time\n",
    "    predictions, true_labels = [], []\n",
    "    for batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "            inputs, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(inputs, attention_mask)\n",
    "            predictions.extend(torch.argmax(outputs, dim=1).cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    testing_end_time = time.time()  # End measuring time\n",
    "    testing_time = testing_end_time - testing_start_time  # Calculate elapsed time for testing\n",
    "\n",
    "    val_f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    report4 = classification_report(true_labels, predictions)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Training Time: {training_time} seconds, Testing Time: {testing_time} seconds\")\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if val_f1 < best_valid_loss:\n",
    "        best_valid_loss = val_f1\n",
    "        counter = 0\n",
    "        #torch.save(model.state_dict(), 'best_model_bert2.pkl')  # Save the model\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            print(report4)\n",
    "            break\n",
    "\n",
    "else:  # This block will be executed if the for loop completes normally, i.e., if early stopping does not occur.\n",
    "    print(\"Training completed.\")\n",
    "    print(report4)  # Print the classification report after the last epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on first sample=70% dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_encodings2, train_df2['labels'].tolist())\n",
    "test_dataset = TextDataset(test_encodings2, test_df2['labels'].tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Time: 114.36370539665222 seconds, Testing Time: 18.12068200111389 seconds\n",
      "Epoch: 1, Training Time: 114.2175395488739 seconds, Testing Time: 17.95778775215149 seconds\n",
      "Epoch: 2, Training Time: 113.63109350204468 seconds, Testing Time: 18.02319598197937 seconds\n",
      "Epoch: 3, Training Time: 114.19601917266846 seconds, Testing Time: 18.204540967941284 seconds\n",
      "Epoch: 4, Training Time: 114.50356435775757 seconds, Testing Time: 18.11637830734253 seconds\n",
      "Epoch: 5, Training Time: 115.2248466014862 seconds, Testing Time: 18.261039972305298 seconds\n",
      "Epoch: 6, Training Time: 115.04927849769592 seconds, Testing Time: 18.096057176589966 seconds\n",
      "Epoch: 7, Training Time: 115.31235361099243 seconds, Testing Time: 18.29442596435547 seconds\n",
      "Epoch: 8, Training Time: 115.63009595870972 seconds, Testing Time: 18.163508653640747 seconds\n",
      "Epoch: 9, Training Time: 115.81140446662903 seconds, Testing Time: 18.332648754119873 seconds\n",
      "Early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.07      0.05        95\n",
      "           1       0.02      0.07      0.03       137\n",
      "           2       0.02      0.03      0.02       126\n",
      "           3       0.02      0.15      0.04        97\n",
      "           4       0.02      0.06      0.03       203\n",
      "           5       0.01      0.04      0.02       115\n",
      "           6       0.02      0.11      0.03        76\n",
      "           7       0.24      0.05      0.08      1450\n",
      "           8       0.02      0.02      0.02       121\n",
      "           9       0.01      0.01      0.01        91\n",
      "          10       0.02      0.03      0.02        59\n",
      "          11       0.02      0.02      0.02       126\n",
      "          12       0.01      0.01      0.01        79\n",
      "          13       0.03      0.01      0.02       146\n",
      "          14       0.03      0.07      0.04       138\n",
      "          15       0.10      0.01      0.02       520\n",
      "          16       0.18      0.06      0.09       601\n",
      "          17       0.03      0.02      0.02       151\n",
      "          18       0.05      0.02      0.03        85\n",
      "          19       0.06      0.05      0.06       220\n",
      "          20       0.00      0.00      0.00       115\n",
      "          21       0.03      0.03      0.03       149\n",
      "          22       0.01      0.01      0.01       169\n",
      "          23       0.08      0.04      0.05       178\n",
      "          24       0.05      0.01      0.02        87\n",
      "          25       0.07      0.12      0.09       220\n",
      "          26       0.06      0.03      0.04       264\n",
      "          27       0.02      0.01      0.01        84\n",
      "          28       0.06      0.02      0.03        55\n",
      "          29       0.04      0.02      0.02        56\n",
      "          30       0.03      0.03      0.03       355\n",
      "          31       0.03      0.06      0.04       136\n",
      "          32       0.05      0.02      0.02       262\n",
      "          33       0.08      0.03      0.04       801\n",
      "          34       0.02      0.04      0.03       191\n",
      "          35       0.25      0.02      0.03        60\n",
      "          36       0.04      0.06      0.04        83\n",
      "          37       0.00      0.00      0.00        65\n",
      "          38       0.03      0.08      0.04       104\n",
      "          39       0.06      0.03      0.04       293\n",
      "          40       0.05      0.04      0.05       136\n",
      "          41       0.05      0.02      0.03       236\n",
      "          42       0.00      0.00      0.00       109\n",
      "          43       0.04      0.01      0.01       140\n",
      "          44       0.03      0.09      0.04       116\n",
      "          45       0.02      0.01      0.01       121\n",
      "          46       0.00      0.00      0.00        77\n",
      "          47       0.05      0.21      0.08        29\n",
      "          48       0.04      0.70      0.08        37\n",
      "          49       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.04      9385\n",
      "   macro avg       0.04      0.05      0.03      9385\n",
      "weighted avg       0.08      0.04      0.04      9385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RNN Tuned MODEL FOR SAMPLE 2\n",
    "import warnings\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class AdjustedRNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.4):\n",
    "        super(AdjustedRNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)  # Use nn.RNN instead of nn.LSTM\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        rnn_out, _ = self.rnn(embedded)  # Use rnn_out instead of lstm_out\n",
    "        # Use the last hidden state for classification\n",
    "        output = self.fc(self.dropout(rnn_out[:, -1, :]))\n",
    "        return output\n",
    "\n",
    "# Instantiate model\n",
    "model = AdjustedRNNModel(vocab_size=tokenizer.vocab_size, embedding_dim=128, hidden_dim=256, output_dim=50, dropout=0.4).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.00002)\n",
    "\n",
    "\n",
    "# Assuming `train_labels` is a list containing all the labels in your training dataset\n",
    "train_labels = [label for batch in train_loader for label in batch['labels'].tolist()]\n",
    "\n",
    "# 1. Compute class distribution\n",
    "class_counts = Counter(train_labels)\n",
    "\n",
    "# 2. Calculate the weights\n",
    "max_count = max(class_counts.values())\n",
    "class_weights = {class_id: max_count / count for class_id, count in class_counts.items()}\n",
    "weights = [class_weights[class_id] for class_id in sorted(class_weights.keys())]\n",
    "\n",
    "weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# 3. Use the weights in the loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()  # use this if data is not imbalanced\n",
    "\n",
    "# Add Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "# Train the model\n",
    "best_f1 = 0.0  # for early stopping based on F1 score\n",
    "\n",
    "for epoch in range(10):\n",
    "    # Measure the time at the start of the epoch\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    training_start_time = time.time()  # Start measuring time\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    training_end_time = time.time()  # End measuring time\n",
    "    training_time = training_end_time - training_start_time  # Calculate elapsed time for training\n",
    "\n",
    "    model.eval()\n",
    "    testing_start_time = time.time()  # Start measuring time\n",
    "    predictions, true_labels = [], []\n",
    "    val_loss = 0  # to compute average validation loss for scheduler\n",
    "\n",
    "    for batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            predictions.extend(torch.argmax(outputs, dim=1).cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    testing_end_time = time.time()  # End measuring time\n",
    "    testing_time = testing_end_time - testing_start_time  # Calculate elapsed time for testing\n",
    "\n",
    "    val_f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    scheduler.step(val_loss / len(test_loader))  # scheduler step based on avg val loss\n",
    "    report5 = classification_report(true_labels, predictions)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Training Time: {training_time} seconds, Testing Time: {testing_time} seconds\")\n",
    "    \n",
    "    # Implementing early stopping based on F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        #torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= 3:\n",
    "            print(\"Early stopping\")\n",
    "            print(report5)\n",
    "            break\n",
    "\n",
    "else:  # This block will be executed if the for loop completes normally, i.e., if early stopping does not occur.\n",
    "    print(\"Training completed.\")\n",
    "    print(report5)  # Print the classification report after the last epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Time: 282.30864095687866 seconds, Testing Time: 37.554548501968384 seconds\n",
      "Epoch: 1, Training Time: 282.4274568557739 seconds, Testing Time: 37.24467945098877 seconds\n",
      "Epoch: 2, Training Time: 284.0395698547363 seconds, Testing Time: 37.68706464767456 seconds\n",
      "Epoch: 3, Training Time: 281.14775109291077 seconds, Testing Time: 37.163119316101074 seconds\n",
      "Epoch: 4, Training Time: 299.72270131111145 seconds, Testing Time: 37.15981864929199 seconds\n",
      "Epoch: 5, Training Time: 281.7582960128784 seconds, Testing Time: 37.40096712112427 seconds\n",
      "Epoch: 6, Training Time: 279.7129969596863 seconds, Testing Time: 37.12498164176941 seconds\n",
      "Epoch: 7, Training Time: 281.18375396728516 seconds, Testing Time: 37.20955753326416 seconds\n",
      "Epoch: 8, Training Time: 281.5672118663788 seconds, Testing Time: 37.38908553123474 seconds\n",
      "Epoch: 9, Training Time: 281.1390874385834 seconds, Testing Time: 37.462230920791626 seconds\n",
      "Training completed.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.61      0.38        95\n",
      "           1       0.23      0.55      0.32       137\n",
      "           2       0.25      0.40      0.31       126\n",
      "           3       0.72      0.75      0.73        97\n",
      "           4       0.22      0.58      0.32       203\n",
      "           5       0.27      0.45      0.34       115\n",
      "           6       0.61      0.53      0.56        76\n",
      "           7       0.50      0.18      0.26      1450\n",
      "           8       0.60      0.73      0.66       121\n",
      "           9       0.76      0.66      0.71        91\n",
      "          10       0.63      0.66      0.64        59\n",
      "          11       0.90      0.87      0.88       126\n",
      "          12       0.40      0.61      0.48        79\n",
      "          13       0.32      0.73      0.45       146\n",
      "          14       0.35      0.66      0.46       138\n",
      "          15       0.54      0.21      0.30       520\n",
      "          16       0.80      0.66      0.72       601\n",
      "          17       0.68      0.77      0.73       151\n",
      "          18       0.60      0.69      0.64        85\n",
      "          19       0.76      0.80      0.78       220\n",
      "          20       0.52      0.82      0.64       115\n",
      "          21       0.69      0.56      0.61       149\n",
      "          22       0.66      0.66      0.66       169\n",
      "          23       0.51      0.48      0.50       178\n",
      "          24       0.24      0.28      0.26        87\n",
      "          25       0.87      0.88      0.87       220\n",
      "          26       0.67      0.73      0.70       264\n",
      "          27       0.36      0.45      0.40        84\n",
      "          28       0.43      0.35      0.38        55\n",
      "          29       0.67      0.91      0.77        56\n",
      "          30       0.51      0.33      0.40       355\n",
      "          31       0.55      0.61      0.58       136\n",
      "          32       0.30      0.38      0.34       262\n",
      "          33       0.58      0.37      0.45       801\n",
      "          34       0.50      0.56      0.53       191\n",
      "          35       0.53      0.70      0.60        60\n",
      "          36       0.56      0.63      0.59        83\n",
      "          37       0.36      0.54      0.43        65\n",
      "          38       0.42      0.80      0.55       104\n",
      "          39       0.63      0.81      0.71       293\n",
      "          40       0.69      0.54      0.61       136\n",
      "          41       0.43      0.39      0.41       236\n",
      "          42       0.58      0.75      0.65       109\n",
      "          43       0.75      0.68      0.71       140\n",
      "          44       0.47      0.72      0.57       116\n",
      "          45       0.45      0.54      0.49       121\n",
      "          46       0.35      0.64      0.45        77\n",
      "          47       0.37      0.66      0.48        29\n",
      "          48       0.39      0.30      0.34        37\n",
      "          49       0.12      0.05      0.07        21\n",
      "\n",
      "    accuracy                           0.51      9385\n",
      "   macro avg       0.51      0.58      0.53      9385\n",
      "weighted avg       0.54      0.51      0.50      9385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##LSTM tined MODEL FOR SAMPLE 1\n",
    "import warnings\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class AdjustedLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.2):\n",
    "        super(AdjustedLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # Use the last hidden state for classification\n",
    "        output = self.fc(self.dropout(lstm_out[:, -1, :]))\n",
    "        return output\n",
    "\n",
    "# Instantiate model\n",
    "model = AdjustedLSTMModel(vocab_size=tokenizer.vocab_size, embedding_dim=128, hidden_dim=256, output_dim=50, dropout=0.2).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(),lr=0.001)\n",
    "\n",
    "# If your dataset is imbalanced, compute class weights\n",
    "# weights = # Compute based on class distribution\n",
    "# criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "\n",
    "# Assuming `train_labels` is a list containing all the labels in your training dataset\n",
    "train_labels = [label for batch in train_loader for label in batch['labels'].tolist()]\n",
    "\n",
    "# 1. Compute class distribution\n",
    "class_counts = Counter(train_labels)\n",
    "\n",
    "# 2. Calculate the weights\n",
    "max_count = max(class_counts.values())\n",
    "class_weights = {class_id: max_count / count for class_id, count in class_counts.items()}\n",
    "weights = [class_weights[class_id] for class_id in sorted(class_weights.keys())]\n",
    "\n",
    "weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# 3. Use the weights in the loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()  # use this if data is not imbalanced\n",
    "\n",
    "# Add Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "# Train the model\n",
    "best_f1 = 0.0  # for early stopping based on F1 score\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    training_start_time = time.time()  # Start measuring time\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    training_end_time = time.time()  # End measuring time\n",
    "    training_time = training_end_time - training_start_time  # Calculate elapsed time for training\n",
    "\n",
    "    model.eval()\n",
    "    testing_start_time = time.time()  # Start measuring time\n",
    "    predictions, true_labels = [], []\n",
    "    val_loss = 0  # to compute average validation loss for scheduler\n",
    "\n",
    "    for batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            predictions.extend(torch.argmax(outputs, dim=1).cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    testing_end_time = time.time()  # End measuring time\n",
    "    testing_time = testing_end_time - testing_start_time  # Calculate elapsed time for testing\n",
    "\n",
    "    val_f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    scheduler.step(val_loss / len(test_loader))  # scheduler step based on avg val loss\n",
    "    report6 = classification_report(true_labels, predictions)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Training Time: {training_time} seconds, Testing Time: {testing_time} seconds\")\n",
    "    \n",
    "    # Implementing early stopping based on F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        #torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= 3:\n",
    "            print(\"Early stopping\")\n",
    "            print(report6)\n",
    "            break\n",
    "\n",
    "else:  # This block will be executed if the for loop completes normally, i.e., if early stopping does not occur.\n",
    "    print(\"Training completed.\")\n",
    "    print(report6)  # Print the classification report after the last epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Time: 334.9066274166107 seconds, Testing Time: 38.58301305770874 seconds\n",
      "Epoch: 1, Training Time: 336.1887230873108 seconds, Testing Time: 38.69426941871643 seconds\n",
      "Epoch: 2, Training Time: 337.580931186676 seconds, Testing Time: 40.988815784454346 seconds\n",
      "Epoch: 3, Training Time: 276.0445907115936 seconds, Testing Time: 7.127222537994385 seconds\n",
      "Epoch: 4, Training Time: 65.3902382850647 seconds, Testing Time: 7.106908559799194 seconds\n",
      "Epoch: 5, Training Time: 65.43103122711182 seconds, Testing Time: 7.102538824081421 seconds\n",
      "Epoch: 6, Training Time: 65.38250827789307 seconds, Testing Time: 7.115699052810669 seconds\n",
      "Epoch: 7, Training Time: 65.39116549491882 seconds, Testing Time: 7.106505870819092 seconds\n",
      "Epoch: 8, Training Time: 65.44267249107361 seconds, Testing Time: 7.130494832992554 seconds\n",
      "Epoch: 9, Training Time: 65.33995389938354 seconds, Testing Time: 7.109229326248169 seconds\n",
      "Training completed.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.63      0.47        95\n",
      "           1       0.45      0.46      0.46       137\n",
      "           2       0.61      0.46      0.52       126\n",
      "           3       0.42      0.70      0.53        97\n",
      "           4       0.54      0.43      0.48       203\n",
      "           5       0.49      0.48      0.48       115\n",
      "           6       0.48      0.55      0.52        76\n",
      "           7       0.57      0.44      0.50      1450\n",
      "           8       0.69      0.75      0.72       121\n",
      "           9       0.84      0.70      0.77        91\n",
      "          10       0.69      0.59      0.64        59\n",
      "          11       0.74      0.75      0.75       126\n",
      "          12       0.58      0.53      0.56        79\n",
      "          13       0.39      0.41      0.40       146\n",
      "          14       0.31      0.49      0.38       138\n",
      "          15       0.50      0.48      0.49       520\n",
      "          16       0.74      0.62      0.68       601\n",
      "          17       0.58      0.62      0.60       151\n",
      "          18       0.62      0.72      0.66        85\n",
      "          19       0.66      0.81      0.73       220\n",
      "          20       0.76      0.75      0.75       115\n",
      "          21       0.64      0.64      0.64       149\n",
      "          22       0.57      0.55      0.56       169\n",
      "          23       0.60      0.51      0.55       178\n",
      "          24       0.24      0.20      0.21        87\n",
      "          25       0.82      0.89      0.85       220\n",
      "          26       0.78      0.72      0.75       264\n",
      "          27       0.51      0.46      0.49        84\n",
      "          28       0.37      0.35      0.36        55\n",
      "          29       0.61      0.77      0.68        56\n",
      "          30       0.39      0.47      0.42       355\n",
      "          31       0.47      0.67      0.55       136\n",
      "          32       0.43      0.45      0.44       262\n",
      "          33       0.48      0.43      0.45       801\n",
      "          34       0.45      0.59      0.51       191\n",
      "          35       0.60      0.67      0.63        60\n",
      "          36       0.62      0.65      0.64        83\n",
      "          37       0.82      0.49      0.62        65\n",
      "          38       0.58      0.73      0.65       104\n",
      "          39       0.64      0.82      0.72       293\n",
      "          40       0.51      0.70      0.59       136\n",
      "          41       0.44      0.50      0.47       236\n",
      "          42       0.79      0.69      0.74       109\n",
      "          43       0.61      0.70      0.65       140\n",
      "          44       0.54      0.53      0.53       116\n",
      "          45       0.55      0.49      0.52       121\n",
      "          46       0.62      0.49      0.55        77\n",
      "          47       0.36      0.41      0.39        29\n",
      "          48       0.42      0.51      0.46        37\n",
      "          49       1.00      0.29      0.44        21\n",
      "\n",
      "    accuracy                           0.56      9385\n",
      "   macro avg       0.57      0.57      0.56      9385\n",
      "weighted avg       0.57      0.56      0.55      9385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##BiLSTM tuned MODEL FOR SAMPLE 1\n",
    "import warnings\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class AdjustedBiLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.2):\n",
    "        super(AdjustedBiLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Add this line to store the hidden_dim as an instance variable\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_dim)  # Multiply by 2 because it's bidirectional\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # Use the last hidden state for classification. We concatenate the last hidden state from both directions\n",
    "        output = self.fc(self.dropout(torch.cat((lstm_out[:, -1, :self.hidden_dim], lstm_out[:, 0, self.hidden_dim:]), dim=1)))\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "model = AdjustedBiLSTMModel(vocab_size=tokenizer.vocab_size, embedding_dim=128, hidden_dim=256, output_dim=50, dropout=0.2).to(device)\n",
    "\n",
    "#model = AdjustedLSTMModel(vocab_size=tokenizer.vocab_size, embedding_dim=128, hidden_dim=256, output_dim=10, dropout=0.2).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(),lr=0.001)\n",
    "\n",
    "# If your dataset is imbalanced, compute class weights\n",
    "# weights = # Compute based on class distribution\n",
    "# criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "\n",
    "# Assuming `train_labels` is a list containing all the labels in your training dataset\n",
    "train_labels = [label for batch in train_loader for label in batch['labels'].tolist()]\n",
    "\n",
    "# 1. Compute class distribution\n",
    "class_counts = Counter(train_labels)\n",
    "\n",
    "# 2. Calculate the weights\n",
    "max_count = max(class_counts.values())\n",
    "class_weights = {class_id: max_count / count for class_id, count in class_counts.items()}\n",
    "weights = [class_weights[class_id] for class_id in sorted(class_weights.keys())]\n",
    "\n",
    "weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# 3. Use the weights in the loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()  # use this if data is not imbalanced\n",
    "\n",
    "# Add Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "# Train the model\n",
    "best_f1 = 0.0  # for early stopping based on F1 score\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    training_start_time = time.time()  # Start measuring time\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    training_end_time = time.time()  # End measuring time\n",
    "    training_time = training_end_time - training_start_time  # Calculate elapsed time for training\n",
    "\n",
    "    model.eval()\n",
    "    testing_start_time = time.time()  # Start measuring time\n",
    "    predictions, true_labels = [], []\n",
    "    val_loss = 0  # to compute average validation loss for scheduler\n",
    "\n",
    "    for batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            predictions.extend(torch.argmax(outputs, dim=1).cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    testing_end_time = time.time()  # End measuring time\n",
    "    testing_time = testing_end_time - testing_start_time  # Calculate elapsed time for testing\n",
    "\n",
    "    val_f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    scheduler.step(val_loss / len(test_loader))  # scheduler step based on avg val loss\n",
    "    report7 = classification_report(true_labels, predictions)\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, Training Time: {training_time} seconds, Testing Time: {testing_time} seconds\")\n",
    "    \n",
    "    # Implementing early stopping based on F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        #torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= 3:\n",
    "            print(\"Early stopping\")\n",
    "            print(report7)\n",
    "            break\n",
    "\n",
    "else:  # This block will be executed if the for loop completes normally, i.e., if early stopping does not occur.\n",
    "    print(\"Training completed.\")\n",
    "    print(report7)  # Print the classification report after the last epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Time: 5067.567667484283 seconds, Testing Time: 537.2134208679199 seconds\n",
      "Epoch: 1, Training Time: 9908.9524102211 seconds, Testing Time: 1077.0321941375732 seconds\n",
      "Epoch: 2, Training Time: 11579.810034036636 seconds, Testing Time: 1047.3660085201263 seconds\n"
     ]
    }
   ],
   "source": [
    "##BERT MODEL FOR SAMPLE 2\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BERTForClassification(50).to(device)  # replace NUM_CLASSES with the number of unique labels in your dataset\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 3\n",
    "best_valid_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    training_start_time = time.time()  # Start measuring time\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        inputs, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "        outputs = model(inputs, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    training_end_time = time.time()  # End measuring time\n",
    "    training_time = training_end_time - training_start_time  # Calculate elapsed time for training\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    testing_start_time = time.time()  # Start measuring time\n",
    "    predictions, true_labels = [], []\n",
    "    for batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "            inputs, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(inputs, attention_mask)\n",
    "            predictions.extend(torch.argmax(outputs, dim=1).cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    testing_end_time = time.time()  # End measuring time\n",
    "    testing_time = testing_end_time - testing_start_time  # Calculate elapsed time for testing\n",
    "\n",
    "    val_f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    report8 = classification_report(true_labels, predictions)\n",
    "    print(f\"Epoch: {epoch}, Training Time: {training_time} seconds, Testing Time: {testing_time} seconds\")\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if val_f1 < best_valid_loss:\n",
    "        best_valid_loss = val_f1\n",
    "        counter = 0\n",
    "        #torch.save(model.state_dict(), 'best_model_bert2.pkl')  # Save the model\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            print(report8)\n",
    "            break\n",
    "\n",
    "else:  # This block will be executed if the for loop completes normally, i.e., if early stopping does not occur.\n",
    "    print(\"Training completed.\")\n",
    "    print(report8)  # Print the classification report after the last epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working on full dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_encodings3, train_df3['labels'].tolist())\n",
    "test_dataset = TextDataset(test_encodings3, test_df3['labels'].tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Time: 29.0020489692688 seconds, Testing Time: 3.5043396949768066 seconds\n",
      "Epoch: 1, Training Time: 28.951069355010986 seconds, Testing Time: 3.5032193660736084 seconds\n",
      "Epoch: 2, Training Time: 28.958198070526123 seconds, Testing Time: 3.505051851272583 seconds\n",
      "Epoch: 3, Training Time: 28.955583095550537 seconds, Testing Time: 3.503267288208008 seconds\n",
      "Epoch: 4, Training Time: 28.830087661743164 seconds, Testing Time: 3.5017645359039307 seconds\n",
      "Epoch: 5, Training Time: 28.56299901008606 seconds, Testing Time: 3.5034854412078857 seconds\n",
      "Epoch: 6, Training Time: 28.769779205322266 seconds, Testing Time: 3.5019419193267822 seconds\n",
      "Epoch: 7, Training Time: 28.95014190673828 seconds, Testing Time: 3.50072979927063 seconds\n",
      "Epoch: 8, Training Time: 28.95990514755249 seconds, Testing Time: 3.50467848777771 seconds\n",
      "Epoch: 9, Training Time: 28.962185621261597 seconds, Testing Time: 3.506392240524292 seconds\n",
      "Training completed.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.03      0.03      0.03       160\n",
      "           1       0.02      0.07      0.04       196\n",
      "           2       0.03      0.04      0.03       186\n",
      "           3       0.03      0.21      0.05       140\n",
      "           4       0.04      0.05      0.05       278\n",
      "           5       0.03      0.14      0.04       194\n",
      "           6       0.04      0.08      0.05       100\n",
      "           7       0.17      0.06      0.09      1969\n",
      "           8       0.02      0.03      0.02       170\n",
      "           9       0.04      0.02      0.03       132\n",
      "          10       0.02      0.03      0.02        93\n",
      "          11       0.03      0.06      0.04       197\n",
      "          12       0.02      0.01      0.01       120\n",
      "          13       0.04      0.03      0.04       228\n",
      "          14       0.04      0.03      0.04       228\n",
      "          15       0.04      0.01      0.01       684\n",
      "          16       0.19      0.14      0.16       834\n",
      "          17       0.01      0.01      0.01       215\n",
      "          18       0.02      0.06      0.03       121\n",
      "          19       0.08      0.10      0.09       308\n",
      "          20       0.03      0.03      0.03       150\n",
      "          21       0.01      0.01      0.01       162\n",
      "          22       0.03      0.05      0.04       237\n",
      "          23       0.03      0.02      0.02       244\n",
      "          24       0.01      0.01      0.01       130\n",
      "          25       0.06      0.11      0.08       334\n",
      "          26       0.11      0.03      0.04       399\n",
      "          27       0.01      0.01      0.01       135\n",
      "          28       0.03      0.01      0.01       103\n",
      "          29       0.00      0.00      0.00        80\n",
      "          30       0.07      0.07      0.07       560\n",
      "          31       0.02      0.01      0.02       219\n",
      "          32       0.06      0.02      0.03       387\n",
      "          33       0.15      0.04      0.06      1079\n",
      "          34       0.06      0.02      0.03       297\n",
      "          35       0.00      0.00      0.00        77\n",
      "          36       0.00      0.00      0.00       134\n",
      "          37       0.05      0.01      0.02        88\n",
      "          38       0.03      0.09      0.04       140\n",
      "          39       0.03      0.01      0.02       428\n",
      "          40       0.03      0.02      0.02       237\n",
      "          41       0.06      0.04      0.05       294\n",
      "          42       0.08      0.03      0.04       174\n",
      "          43       0.03      0.01      0.02       222\n",
      "          44       0.02      0.03      0.02       155\n",
      "          45       0.03      0.01      0.02       141\n",
      "          46       0.02      0.03      0.02       118\n",
      "          47       0.02      0.20      0.03        46\n",
      "          48       0.04      0.56      0.07        54\n",
      "          49       0.00      0.00      0.00        30\n",
      "\n",
      "    accuracy                           0.05     13407\n",
      "   macro avg       0.04      0.05      0.03     13407\n",
      "weighted avg       0.08      0.05      0.05     13407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RNN Tuned MODEL FOR SAMPLE 3\n",
    "import warnings\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class AdjustedRNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.4):\n",
    "        super(AdjustedRNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)  # Use nn.RNN instead of nn.LSTM\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        rnn_out, _ = self.rnn(embedded)  # Use rnn_out instead of lstm_out\n",
    "        # Use the last hidden state for classification\n",
    "        output = self.fc(self.dropout(rnn_out[:, -1, :]))\n",
    "        return output\n",
    "\n",
    "# Instantiate model\n",
    "model = AdjustedRNNModel(vocab_size=tokenizer.vocab_size, embedding_dim=128, hidden_dim=256, output_dim=50, dropout=0.4).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.00002)\n",
    "\n",
    "\n",
    "# Assuming `train_labels` is a list containing all the labels in your training dataset\n",
    "train_labels = [label for batch in train_loader for label in batch['labels'].tolist()]\n",
    "\n",
    "# 1. Compute class distribution\n",
    "class_counts = Counter(train_labels)\n",
    "\n",
    "# 2. Calculate the weights\n",
    "max_count = max(class_counts.values())\n",
    "class_weights = {class_id: max_count / count for class_id, count in class_counts.items()}\n",
    "weights = [class_weights[class_id] for class_id in sorted(class_weights.keys())]\n",
    "\n",
    "weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# 3. Use the weights in the loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()  # use this if data is not imbalanced\n",
    "\n",
    "# Add Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "# Train the model\n",
    "best_f1 = 0.0  # for early stopping based on F1 score\n",
    "\n",
    "for epoch in range(10):\n",
    "    # Measure the time at the start of the epoch\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    training_start_time = time.time()  # Start measuring time\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    training_end_time = time.time()  # End measuring time\n",
    "    training_time = training_end_time - training_start_time  # Calculate elapsed time for training\n",
    "\n",
    "    model.eval()\n",
    "    testing_start_time = time.time()  # Start measuring time\n",
    "    predictions, true_labels = [], []\n",
    "    val_loss = 0  # to compute average validation loss for scheduler\n",
    "\n",
    "    for batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            predictions.extend(torch.argmax(outputs, dim=1).cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    testing_end_time = time.time()  # End measuring time\n",
    "    testing_time = testing_end_time - testing_start_time  # Calculate elapsed time for testing\n",
    "\n",
    "    val_f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    scheduler.step(val_loss / len(test_loader))  # scheduler step based on avg val loss\n",
    "    report9 = classification_report(true_labels, predictions)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Training Time: {training_time} seconds, Testing Time: {testing_time} seconds\")\n",
    "    \n",
    "    # Implementing early stopping based on F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), '/workspace/outputs/simplernn_model2.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= 3:\n",
    "            print(\"Early stopping\")\n",
    "            print(report9)\n",
    "            break\n",
    "\n",
    "else:  # This block will be executed if the for loop completes normally, i.e., if early stopping does not occur.\n",
    "    print(\"Training completed.\")\n",
    "    print(report9)  # Print the classification report after the last epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Time: 87.47678446769714 seconds, Testing Time: 9.961474418640137 seconds\n",
      "Epoch: 1, Training Time: 87.53887391090393 seconds, Testing Time: 9.962533473968506 seconds\n",
      "Epoch: 2, Training Time: 87.53276824951172 seconds, Testing Time: 9.965057849884033 seconds\n",
      "Epoch: 3, Training Time: 87.57044863700867 seconds, Testing Time: 9.96553635597229 seconds\n",
      "Epoch: 4, Training Time: 87.55779552459717 seconds, Testing Time: 9.96853232383728 seconds\n",
      "Epoch: 5, Training Time: 87.6807816028595 seconds, Testing Time: 9.964990854263306 seconds\n",
      "Epoch: 6, Training Time: 87.55305337905884 seconds, Testing Time: 9.96816635131836 seconds\n",
      "Epoch: 7, Training Time: 87.68879103660583 seconds, Testing Time: 9.970109701156616 seconds\n",
      "Epoch: 8, Training Time: 87.74234414100647 seconds, Testing Time: 9.967970609664917 seconds\n",
      "Epoch: 9, Training Time: 87.57272601127625 seconds, Testing Time: 9.968755960464478 seconds\n",
      "Training completed.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.82      0.67       160\n",
      "           1       0.54      0.73      0.62       196\n",
      "           2       0.60      0.75      0.66       186\n",
      "           3       0.72      0.83      0.77       140\n",
      "           4       0.51      0.70      0.59       278\n",
      "           5       0.65      0.58      0.62       194\n",
      "           6       0.53      0.76      0.63       100\n",
      "           7       0.70      0.44      0.54      1969\n",
      "           8       0.79      0.76      0.77       170\n",
      "           9       0.85      0.80      0.82       132\n",
      "          10       0.72      0.89      0.79        93\n",
      "          11       0.85      0.86      0.86       197\n",
      "          12       0.40      0.80      0.53       120\n",
      "          13       0.61      0.68      0.64       228\n",
      "          14       0.69      0.72      0.70       228\n",
      "          15       0.73      0.41      0.53       684\n",
      "          16       0.75      0.78      0.76       834\n",
      "          17       0.80      0.77      0.78       215\n",
      "          18       0.74      0.83      0.78       121\n",
      "          19       0.76      0.84      0.80       308\n",
      "          20       0.77      0.86      0.81       150\n",
      "          21       0.58      0.73      0.64       162\n",
      "          22       0.69      0.71      0.70       237\n",
      "          23       0.67      0.72      0.70       244\n",
      "          24       0.28      0.43      0.34       130\n",
      "          25       0.95      0.86      0.91       334\n",
      "          26       0.82      0.78      0.80       399\n",
      "          27       0.49      0.56      0.52       135\n",
      "          28       0.37      0.41      0.39       103\n",
      "          29       0.87      0.85      0.86        80\n",
      "          30       0.52      0.55      0.54       560\n",
      "          31       0.60      0.78      0.68       219\n",
      "          32       0.40      0.59      0.48       387\n",
      "          33       0.72      0.52      0.60      1079\n",
      "          34       0.68      0.63      0.65       297\n",
      "          35       0.64      0.88      0.74        77\n",
      "          36       0.70      0.75      0.72       134\n",
      "          37       0.77      0.66      0.71        88\n",
      "          38       0.71      0.86      0.78       140\n",
      "          39       0.78      0.84      0.81       428\n",
      "          40       0.59      0.72      0.65       237\n",
      "          41       0.52      0.60      0.56       294\n",
      "          42       0.74      0.84      0.79       174\n",
      "          43       0.71      0.83      0.77       222\n",
      "          44       0.64      0.82      0.72       155\n",
      "          45       0.53      0.72      0.61       141\n",
      "          46       0.57      0.64      0.60       118\n",
      "          47       0.98      0.87      0.92        46\n",
      "          48       0.88      0.96      0.92        54\n",
      "          49       0.97      0.97      0.97        30\n",
      "\n",
      "    accuracy                           0.66     13407\n",
      "   macro avg       0.67      0.73      0.69     13407\n",
      "weighted avg       0.68      0.66      0.66     13407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##LSTM tined MODEL FOR SAMPLE 3\n",
    "import warnings\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class AdjustedLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.2):\n",
    "        super(AdjustedLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # Use the last hidden state for classification\n",
    "        output = self.fc(self.dropout(lstm_out[:, -1, :]))\n",
    "        return output\n",
    "\n",
    "# Instantiate model\n",
    "model = AdjustedLSTMModel(vocab_size=tokenizer.vocab_size, embedding_dim=128, hidden_dim=256, output_dim=50, dropout=0.2).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(),lr=0.001)\n",
    "\n",
    "# If your dataset is imbalanced, compute class weights\n",
    "# weights = # Compute based on class distribution\n",
    "# criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "\n",
    "# Assuming `train_labels` is a list containing all the labels in your training dataset\n",
    "train_labels = [label for batch in train_loader for label in batch['labels'].tolist()]\n",
    "\n",
    "# 1. Compute class distribution\n",
    "class_counts = Counter(train_labels)\n",
    "\n",
    "# 2. Calculate the weights\n",
    "max_count = max(class_counts.values())\n",
    "class_weights = {class_id: max_count / count for class_id, count in class_counts.items()}\n",
    "weights = [class_weights[class_id] for class_id in sorted(class_weights.keys())]\n",
    "\n",
    "weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# 3. Use the weights in the loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()  # use this if data is not imbalanced\n",
    "\n",
    "# Add Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "# Train the model\n",
    "best_f1 = 0.0  # for early stopping based on F1 score\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    training_start_time = time.time()  # Start measuring time\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    training_end_time = time.time()  # End measuring time\n",
    "    training_time = training_end_time - training_start_time  # Calculate elapsed time for training\n",
    "\n",
    "    model.eval()\n",
    "    testing_start_time = time.time()  # Start measuring time\n",
    "    predictions, true_labels = [], []\n",
    "    val_loss = 0  # to compute average validation loss for scheduler\n",
    "\n",
    "    for batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            predictions.extend(torch.argmax(outputs, dim=1).cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    testing_end_time = time.time()  # End measuring time\n",
    "    testing_time = testing_end_time - testing_start_time  # Calculate elapsed time for testing\n",
    "\n",
    "    val_f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    scheduler.step(val_loss / len(test_loader))  # scheduler step based on avg val loss\n",
    "    report10 = classification_report(true_labels, predictions)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Training Time: {training_time} seconds, Testing Time: {testing_time} seconds\")\n",
    "    \n",
    "    # Implementing early stopping based on F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), '/workspace/outputs/lstm_model2.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= 3:\n",
    "            print(\"Early stopping\")\n",
    "            print(report10)\n",
    "            break\n",
    "\n",
    "else:  # This block will be executed if the for loop completes normally, i.e., if early stopping does not occur.\n",
    "    print(\"Training completed.\")\n",
    "    print(report10)  # Print the classification report after the last epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Time: 93.64659595489502 seconds, Testing Time: 10.138993740081787 seconds\n",
      "Epoch: 1, Training Time: 93.73997354507446 seconds, Testing Time: 10.150611639022827 seconds\n",
      "Epoch: 2, Training Time: 93.68841218948364 seconds, Testing Time: 10.130943059921265 seconds\n",
      "Epoch: 3, Training Time: 93.75651669502258 seconds, Testing Time: 10.119308710098267 seconds\n",
      "Epoch: 4, Training Time: 93.71237206459045 seconds, Testing Time: 10.127362251281738 seconds\n",
      "Epoch: 5, Training Time: 93.73949456214905 seconds, Testing Time: 10.132631778717041 seconds\n",
      "Epoch: 6, Training Time: 93.7659215927124 seconds, Testing Time: 10.141343832015991 seconds\n",
      "Epoch: 7, Training Time: 93.7303318977356 seconds, Testing Time: 10.140599489212036 seconds\n",
      "Epoch: 8, Training Time: 93.7246823310852 seconds, Testing Time: 10.1498441696167 seconds\n",
      "Epoch: 9, Training Time: 93.70581793785095 seconds, Testing Time: 10.124911069869995 seconds\n",
      "Training completed.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65       160\n",
      "           1       0.68      0.67      0.68       196\n",
      "           2       0.60      0.67      0.64       186\n",
      "           3       0.83      0.78      0.80       140\n",
      "           4       0.53      0.63      0.57       278\n",
      "           5       0.66      0.55      0.60       194\n",
      "           6       0.46      0.66      0.54       100\n",
      "           7       0.72      0.49      0.58      1969\n",
      "           8       0.70      0.76      0.73       170\n",
      "           9       0.73      0.81      0.77       132\n",
      "          10       0.64      0.80      0.71        93\n",
      "          11       0.83      0.82      0.82       197\n",
      "          12       0.64      0.64      0.64       120\n",
      "          13       0.60      0.60      0.60       228\n",
      "          14       0.64      0.65      0.64       228\n",
      "          15       0.55      0.59      0.57       684\n",
      "          16       0.80      0.73      0.77       834\n",
      "          17       0.70      0.66      0.68       215\n",
      "          18       0.71      0.78      0.74       121\n",
      "          19       0.81      0.82      0.82       308\n",
      "          20       0.76      0.77      0.76       150\n",
      "          21       0.68      0.70      0.69       162\n",
      "          22       0.68      0.68      0.68       237\n",
      "          23       0.52      0.64      0.57       244\n",
      "          24       0.29      0.38      0.33       130\n",
      "          25       0.90      0.85      0.88       334\n",
      "          26       0.83      0.80      0.81       399\n",
      "          27       0.48      0.67      0.56       135\n",
      "          28       0.46      0.31      0.37       103\n",
      "          29       0.77      0.84      0.80        80\n",
      "          30       0.47      0.57      0.52       560\n",
      "          31       0.55      0.68      0.61       219\n",
      "          32       0.46      0.54      0.49       387\n",
      "          33       0.59      0.55      0.57      1079\n",
      "          34       0.55      0.70      0.61       297\n",
      "          35       0.80      0.84      0.82        77\n",
      "          36       0.54      0.72      0.62       134\n",
      "          37       0.76      0.60      0.67        88\n",
      "          38       0.68      0.83      0.75       140\n",
      "          39       0.81      0.82      0.81       428\n",
      "          40       0.63      0.68      0.65       237\n",
      "          41       0.50      0.50      0.50       294\n",
      "          42       0.79      0.87      0.83       174\n",
      "          43       0.83      0.78      0.80       222\n",
      "          44       0.73      0.73      0.73       155\n",
      "          45       0.72      0.68      0.70       141\n",
      "          46       0.48      0.64      0.55       118\n",
      "          47       0.46      0.35      0.40        46\n",
      "          48       0.49      0.61      0.55        54\n",
      "          49       0.96      0.80      0.87        30\n",
      "\n",
      "    accuracy                           0.65     13407\n",
      "   macro avg       0.65      0.68      0.66     13407\n",
      "weighted avg       0.66      0.65      0.65     13407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##BiLSTM tuned MODEL FOR SAMPLE 3\n",
    "import warnings\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class AdjustedBiLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.2):\n",
    "        super(AdjustedBiLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Add this line to store the hidden_dim as an instance variable\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_dim)  # Multiply by 2 because it's bidirectional\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # Use the last hidden state for classification. We concatenate the last hidden state from both directions\n",
    "        output = self.fc(self.dropout(torch.cat((lstm_out[:, -1, :self.hidden_dim], lstm_out[:, 0, self.hidden_dim:]), dim=1)))\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "model = AdjustedBiLSTMModel(vocab_size=tokenizer.vocab_size, embedding_dim=128, hidden_dim=256, output_dim=50, dropout=0.2).to(device)\n",
    "\n",
    "#model = AdjustedLSTMModel(vocab_size=tokenizer.vocab_size, embedding_dim=128, hidden_dim=256, output_dim=10, dropout=0.2).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(),lr=0.001)\n",
    "\n",
    "# If your dataset is imbalanced, compute class weights\n",
    "# weights = # Compute based on class distribution\n",
    "# criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "\n",
    "# Assuming `train_labels` is a list containing all the labels in your training dataset\n",
    "train_labels = [label for batch in train_loader for label in batch['labels'].tolist()]\n",
    "\n",
    "# 1. Compute class distribution\n",
    "class_counts = Counter(train_labels)\n",
    "\n",
    "# 2. Calculate the weights\n",
    "max_count = max(class_counts.values())\n",
    "class_weights = {class_id: max_count / count for class_id, count in class_counts.items()}\n",
    "weights = [class_weights[class_id] for class_id in sorted(class_weights.keys())]\n",
    "\n",
    "weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# 3. Use the weights in the loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()  # use this if data is not imbalanced\n",
    "\n",
    "# Add Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "# Train the model\n",
    "best_f1 = 0.0  # for early stopping based on F1 score\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    training_start_time = time.time()  # Start measuring time\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    training_end_time = time.time()  # End measuring time\n",
    "    training_time = training_end_time - training_start_time  # Calculate elapsed time for training\n",
    "\n",
    "    model.eval()\n",
    "    testing_start_time = time.time()  # Start measuring time\n",
    "    predictions, true_labels = [], []\n",
    "    val_loss = 0  # to compute average validation loss for scheduler\n",
    "\n",
    "    for batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            predictions.extend(torch.argmax(outputs, dim=1).cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    testing_end_time = time.time()  # End measuring time\n",
    "    testing_time = testing_end_time - testing_start_time  # Calculate elapsed time for testing\n",
    "\n",
    "    val_f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    scheduler.step(val_loss / len(test_loader))  # scheduler step based on avg val loss\n",
    "    report11 = classification_report(true_labels, predictions)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Training Time: {training_time} seconds, Testing Time: {testing_time} seconds\")\n",
    "    \n",
    "    # Implementing early stopping based on F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), '/workspace/outputs/bilstm_model2.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= 3:\n",
    "            print(\"Early stopping\")\n",
    "            print(report11)\n",
    "            break\n",
    "\n",
    "else:  # This block will be executed if the for loop completes normally, i.e., if early stopping does not occur.\n",
    "    print(\"Training completed.\")\n",
    "    print(report11)  # Print the classification report after the last epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Time: 6347.1341671943665 seconds, Testing Time: 602.6011655330658 seconds\n",
      "Epoch: 1, Training Time: 6302.424460172653 seconds, Testing Time: 595.2031095027924 seconds\n",
      "Epoch: 2, Training Time: 6327.777872562408 seconds, Testing Time: 594.8712770938873 seconds\n",
      "Epoch: 3, Training Time: 6358.15244603157 seconds, Testing Time: 601.5906183719635 seconds\n",
      "Early stopping triggered.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.84      0.78       160\n",
      "           1       0.96      0.72      0.82       196\n",
      "           2       0.90      0.75      0.82       186\n",
      "           3       0.96      0.89      0.93       140\n",
      "           4       0.67      0.79      0.73       278\n",
      "           5       0.88      0.62      0.73       194\n",
      "           6       0.75      0.73      0.74       100\n",
      "           7       0.82      0.85      0.84      1969\n",
      "           8       0.89      0.86      0.88       170\n",
      "           9       0.86      0.82      0.84       132\n",
      "          10       0.98      0.85      0.91        93\n",
      "          11       0.87      0.95      0.91       197\n",
      "          12       0.86      0.78      0.82       120\n",
      "          13       0.84      0.70      0.77       228\n",
      "          14       0.70      0.86      0.77       228\n",
      "          15       0.72      0.70      0.71       684\n",
      "          16       0.86      0.80      0.83       834\n",
      "          17       0.88      0.86      0.87       215\n",
      "          18       0.84      0.90      0.87       121\n",
      "          19       0.87      0.90      0.88       308\n",
      "          20       0.85      0.88      0.87       150\n",
      "          21       0.84      0.82      0.83       162\n",
      "          22       0.81      0.71      0.76       237\n",
      "          23       0.74      0.80      0.77       244\n",
      "          24       0.41      0.38      0.39       130\n",
      "          25       0.99      0.90      0.94       334\n",
      "          26       0.85      0.94      0.90       399\n",
      "          27       0.56      0.80      0.66       135\n",
      "          28       0.62      0.24      0.35       103\n",
      "          29       0.89      0.93      0.91        80\n",
      "          30       0.80      0.69      0.74       560\n",
      "          31       0.89      0.63      0.73       219\n",
      "          32       0.63      0.78      0.70       387\n",
      "          33       0.75      0.88      0.81      1079\n",
      "          34       0.92      0.65      0.76       297\n",
      "          35       0.81      0.83      0.82        77\n",
      "          36       0.75      0.87      0.81       134\n",
      "          37       0.84      0.78      0.81        88\n",
      "          38       0.76      0.89      0.82       140\n",
      "          39       0.87      0.90      0.88       428\n",
      "          40       0.80      0.77      0.78       237\n",
      "          41       0.74      0.73      0.74       294\n",
      "          42       0.94      0.83      0.88       174\n",
      "          43       0.88      0.95      0.91       222\n",
      "          44       0.91      0.75      0.83       155\n",
      "          45       0.74      0.79      0.77       141\n",
      "          46       0.72      0.80      0.76       118\n",
      "          47       0.98      0.91      0.94        46\n",
      "          48       0.93      0.96      0.95        54\n",
      "          49       0.94      0.97      0.95        30\n",
      "\n",
      "    accuracy                           0.81     13407\n",
      "   macro avg       0.82      0.80      0.80     13407\n",
      "weighted avg       0.81      0.81      0.80     13407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##BERT MODEL FOR SAMPLE 1\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BERTForClassification(50).to(device)  # replace NUM_CLASSES with the number of unique labels in your dataset\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 3\n",
    "best_valid_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    training_start_time = time.time()  # Start measuring time\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        inputs, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "        outputs = model(inputs, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    training_end_time = time.time()  # End measuring time\n",
    "    training_time = training_end_time - training_start_time  # Calculate elapsed time for training\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    testing_start_time = time.time()  # Start measuring time\n",
    "    predictions, true_labels = [], []\n",
    "    for batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "            inputs, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(inputs, attention_mask)\n",
    "            predictions.extend(torch.argmax(outputs, dim=1).cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    testing_end_time = time.time()  # End measuring time\n",
    "    testing_time = testing_end_time - testing_start_time  # Calculate elapsed time for testing\n",
    "\n",
    "    val_f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    report12 = classification_report(true_labels, predictions)\n",
    "    print(f\"Epoch: {epoch}, Training Time: {training_time} seconds, Testing Time: {testing_time} seconds\")\n",
    "    # Early stopping logic\n",
    "    if val_f1 < best_valid_loss:\n",
    "        best_valid_loss = val_f1\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), '/workspace/outputs/Bert_model2.pth')  # Save the model\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            print(report12)\n",
    "            break\n",
    "\n",
    "else:  # This block will be executed if the for loop completes normally, i.e., if early stopping does not occur.\n",
    "    print(\"Training completed.\")\n",
    "    print(report12)  # Print the classification report after the last epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
